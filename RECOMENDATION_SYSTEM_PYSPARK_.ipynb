{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "****ML ALGORITHM IN PYSPARK****"
      ],
      "metadata": {
        "id": "VHdVQIB7TTp1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtA9dsQChTGD"
      },
      "source": [
        "**Recommender System using Pyspark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djIbKQtOhare"
      },
      "source": [
        "Collaborative filtering is implemented by the machine learning library Spark MLlib using Alternating Least Squares."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXmMDzuthnGK"
      },
      "source": [
        "Collaborative filtering involves making predictions (filtering) about a user’s interests by compiling preferences or taste data from numerous users (collaborating). The essential premise is that, if two users A and B share the same opinion on a subject, A is more likely to share B’s opinion on a related but unrelated subject, x, than the opinion of a randomly selected user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBbV1kG6GCYu",
        "outputId": "68efbfdd-f653-445b-fdef-2fba84609800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.or\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "\r0% [2 InRelease 15.6 kB/114 kB 14%] [Waiting for headers] [Waiting for headers]\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "\r0% [2 InRelease 15.6 kB/114 kB 14%] [Waiting for headers] [3 InRelease 3,622 B/\r0% [2 InRelease 15.6 kB/114 kB 14%] [Waiting for headers] [Waiting for headers]\r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]\n",
            "\r0% [2 InRelease 21.4 kB/114 kB 19%] [Waiting for headers] [Waiting for headers]\r                                                                               \rGet:5 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1,084 kB]\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,677 kB]\n",
            "Hit:13 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,347 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,372 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.0 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,866 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,072 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2,536 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.3 kB]\n",
            "Fetched 15.4 MB in 2s (6,532 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "# Download Java Virtual Machine (JVM)\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N97SJpzoGGGR"
      },
      "outputs": [],
      "source": [
        "# Download Spark\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "# Unzip the file\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfwDLRMJGQVi"
      },
      "outputs": [],
      "source": [
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nlh3eje2GUTf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = '/content/spark-3.2.1-bin-hadoop3.2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gvp_QFHGYsM",
        "outputId": "57ae9725-1acc-464f-e468-20128d5b02ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "book_ratings.csv  spark-3.2.1-bin-hadoop3.2\n",
            "sample_data\t  spark-3.2.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiYmraQaGaLU"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kJU4k3k4GeuB",
        "outputId": "0edca934-67ae-4e58-a6dc-c71dd3500bf8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.2.1-bin-hadoop3.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Check the location for Spark\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "orgDyiWNgPVK",
        "outputId": "95dc7997-9ef4-4e1c-d5b8-1e14f21bc2e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ca29b6c9780>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://39dcec3dfffb:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Recommender</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#importing the required pyspark library\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "\n",
        "#Setup Spark Session\n",
        "spark = SparkSession.builder.appName('Recommender').getOrCreate()\n",
        "spark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQEwZNcYgUih",
        "outputId": "e000f520-17a4-4276-f8e0-77b17a06537d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+------+\n",
            "|book_id|user_id|rating|\n",
            "+-------+-------+------+\n",
            "|      1|    314|     5|\n",
            "|      1|    439|     3|\n",
            "|      1|    588|     5|\n",
            "|      1|   1169|     4|\n",
            "|      1|   1185|     4|\n",
            "+-------+-------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#CSV file can be downloaded from the link mentioned above.\n",
        "data = spark.read.csv('book_ratings.csv',\n",
        "\t\t\t\t\tinferSchema=True,header=True)\n",
        "\n",
        "data.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of rows\n",
        "num_rows = data.count()\n",
        "\n",
        "# Get the number of columns\n",
        "num_cols = len(data.columns)\n",
        "\n",
        "# Print the shape of the DataFrame\n",
        "print(\"Number of rows:\", num_rows)\n",
        "print(\"Number of columns:\", num_cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIwL9jZi2sEZ",
        "outputId": "6cf48e29-e418-4557-e03a-88cdc310b3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 981756\n",
            "Number of columns: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv0YPMsPgYVA",
        "outputId": "a484d0f5-b56b-42f1-c37e-247322adc029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-----------------+------------------+\n",
            "|summary|           book_id|          user_id|            rating|\n",
            "+-------+------------------+-----------------+------------------+\n",
            "|  count|            254812|           254812|            254812|\n",
            "|   mean|1274.9756408646374|24688.24237477042| 3.825373216332041|\n",
            "| stddev|  736.058995940083|14983.10160239284|1.0099376939054863|\n",
            "|    min|                 1|                1|                 1|\n",
            "|    max|              2550|            53424|                 5|\n",
            "+-------+------------------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.describe().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sysvseKRgcVw"
      },
      "outputs": [],
      "source": [
        "# Dividing the data using random split into train_data and test_data\n",
        "# in 80% and 20% respectively\n",
        "train_data, test_data = data.randomSplit([0.8, 0.2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q6xXRIxge1d"
      },
      "outputs": [],
      "source": [
        "# Build the recommendation model using ALS on the training data\n",
        "als = ALS(maxIter=5,\n",
        "\t\tregParam=0.01,\n",
        "\t\tuserCol=\"user_id\",\n",
        "\t\titemCol=\"book_id\",\n",
        "\t\tratingCol=\"rating\")\n",
        "\n",
        "#Fitting the model on the train_data\n",
        "model = als.fit(train_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa4VXSEPgkan",
        "outputId": "1b0d23d8-0f7f-47db-df67-33435eae69a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+------+----------+\n",
            "|book_id|user_id|rating|prediction|\n",
            "+-------+-------+------+----------+\n",
            "|      2|   9731|     4| 3.7728379|\n",
            "|      1|  16913|     5| 4.1102757|\n",
            "|      1|  32305|     5|  4.497166|\n",
            "|      2|  11868|     5|   3.95726|\n",
            "|      1|   6630|     5|  4.614361|\n",
            "|      2|  13794|     1|  3.255959|\n",
            "|      1|  18361|     4|  4.434146|\n",
            "|      1|  21487|     4|  4.294687|\n",
            "|      1|  25214|     4| 4.4809976|\n",
            "|      1|  25164|     4|  4.033773|\n",
            "|      1|  31001|     4|  4.667442|\n",
            "|      2|   1169|     3| 3.7644792|\n",
            "|      2|   6063|     1|  3.644886|\n",
            "|      1|    314|     5| 4.3220677|\n",
            "|      2|  10509|     2|  4.251693|\n",
            "|      2|  12874|     4|  4.716813|\n",
            "|      1|  51838|     5| 4.5926948|\n",
            "|      1|  33890|     3|  3.960748|\n",
            "|      1|  39423|     3| 3.7693806|\n",
            "|      2|  14372|     3|  4.600501|\n",
            "+-------+-------+------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model by computing the RMSE on the test data\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "#Displaying predictions calculated by the model\n",
        "predictions.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_ah4qkpgpgX",
        "outputId": "d57d3922-e6fe-4211-edf5-3641980e1389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root-mean-square error = nan\n"
          ]
        }
      ],
      "source": [
        "#Printing and calculating RMSE\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root-mean-square error = \" + str(rmse))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FixTFNwzgxSm",
        "outputId": "dc5d9630-9a2b-45da-b378-2477816abbcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+\n",
            "|book_id|user_id|\n",
            "+-------+-------+\n",
            "|      7|   5461|\n",
            "|     15|   5461|\n",
            "|     43|   5461|\n",
            "|     48|   5461|\n",
            "|     66|   5461|\n",
            "|    111|   5461|\n",
            "|    116|   5461|\n",
            "|    117|   5461|\n",
            "|    118|   5461|\n",
            "|    121|   5461|\n",
            "|    130|   5461|\n",
            "|    148|   5461|\n",
            "|    222|   5461|\n",
            "|    255|   5461|\n",
            "|    306|   5461|\n",
            "|    321|   5461|\n",
            "|    339|   5461|\n",
            "|    401|   5461|\n",
            "|    454|   5461|\n",
            "|    489|   5461|\n",
            "+-------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Filtering user with user id \"5461\" with book id on which it has given the reviews\n",
        "user1 = test_data.filter(test_data['user_id']==5461).select(['book_id','user_id'])\n",
        "\n",
        "#Displaying user1 data\n",
        "user1.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_7ZAz45g01D",
        "outputId": "4f124ad9-a26d-4214-e16d-abfc80b7ac86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+----------+\n",
            "|book_id|user_id|prediction|\n",
            "+-------+-------+----------+\n",
            "|    339|   5461| 4.7859526|\n",
            "|    489|   5461| 4.7581034|\n",
            "|    561|   5461| 4.6387863|\n",
            "|   1266|   5461| 4.6110306|\n",
            "|    306|   5461|   4.55613|\n",
            "|    401|   5461|   4.49409|\n",
            "|     15|   5461| 4.4881763|\n",
            "|    117|   5461| 4.4501696|\n",
            "|    111|   5461|  4.285496|\n",
            "|    148|   5461| 4.2722573|\n",
            "|    222|   5461| 4.2612677|\n",
            "|     43|   5461| 4.2518234|\n",
            "|     48|   5461|  4.203925|\n",
            "|     66|   5461| 4.1053185|\n",
            "|    639|   5461| 4.0750923|\n",
            "|   1566|   5461| 3.9123073|\n",
            "|    121|   5461| 3.8474429|\n",
            "|    130|   5461| 3.8462512|\n",
            "|    118|   5461| 3.8150206|\n",
            "|    731|   5461| 3.7864227|\n",
            "+-------+-------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Traning and evaluating for user1 with our model trained with the help of training data\n",
        "recommendations = model.transform(user1)\n",
        "\n",
        "#Displaying the predictions of books for user1\n",
        "recommendations.orderBy('prediction',ascending=False).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kBc6HsmqXWJ"
      },
      "source": [
        "**random forest and decision tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "zKj7D7KoqP-Y",
        "outputId": "1608935b-7cb0-4d65-f9aa-d73e36f32d90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=e2accb5c72e546722e4e0b03a5d10ebc8afebe85118b4615e4cbfa6a5ee82a34\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyspark"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go0D9DOYuNhy",
        "outputId": "3816de06-4780-4d2a-db89-5c2ed412a0a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree RMSE: 1.10\n",
            "+-------+-------+------+----------+\n",
            "|user_id|book_id|rating|prediction|\n",
            "+-------+-------+------+----------+\n",
            "|   5461|      1|     3|       5.0|\n",
            "|   5461|      2|     4|       5.0|\n",
            "|   5461|      3|     2|       5.0|\n",
            "|   5461|      5|     5|       5.0|\n",
            "|   5461|      7|     5|       5.0|\n",
            "|   5461|      8|     4|       5.0|\n",
            "|   5461|      9|     4|       5.0|\n",
            "|   5461|     10|     3|       5.0|\n",
            "|   5461|     11|     4|       5.0|\n",
            "|   5461|     14|     4|       5.0|\n",
            "|   5461|     15|     5|       5.0|\n",
            "|   5461|     16|     4|       5.0|\n",
            "|   5461|     19|     5|       5.0|\n",
            "|   5461|     22|     4|       5.0|\n",
            "|   5461|     28|     5|       5.0|\n",
            "|   5461|     31|     4|       5.0|\n",
            "|   5461|     32|     5|       5.0|\n",
            "|   5461|     33|     4|       5.0|\n",
            "|   5461|     35|     4|       5.0|\n",
            "|   5461|     37|     4|       5.0|\n",
            "+-------+-------+------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"BookPrediction\").getOrCreate()\n",
        "\n",
        "# Create the feature vector assembler\n",
        "featureCols = [\"book_id\", \"user_id\"]\n",
        "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
        "\n",
        "# Transform the data using the feature vector assembler\n",
        "dataAssembled = assembler.transform(data)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "(trainingData, testData) = dataAssembled.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Train a Decision Tree model\n",
        "dt = DecisionTreeClassifier(labelCol=\"rating\", featuresCol=\"features\")\n",
        "dtModel = dt.fit(trainingData)\n",
        "\n",
        "# Make predictions on the test data using the Decision Tree model\n",
        "dtPredictions = dtModel.transform(testData)\n",
        "\n",
        "# Evaluate the model using RegressionEvaluator\n",
        "evaluator = RegressionEvaluator(labelCol=\"rating\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "dtRMSE = evaluator.evaluate(dtPredictions)\n",
        "print(\"Decision Tree RMSE: {:.2f}\".format(dtRMSE))\n",
        "\n",
        "# Single user recommendation example\n",
        "user_id = 5461\n",
        "\n",
        "# Filter the data for the given user\n",
        "user_data = data.filter(data.user_id == user_id)\n",
        "\n",
        "# Assemble the features for the user data\n",
        "user_data_assembled = assembler.transform(user_data)\n",
        "\n",
        "# Make predictions for the user using the Decision Tree model\n",
        "user_predictions = dtModel.transform(user_data_assembled)\n",
        "\n",
        "# Show the recommendations for the user\n",
        "user_recommendations = user_predictions.select(\"user_id\", \"book_id\", \"prediction\")\n",
        "user_actual_ratings = user_data.join(user_recommendations, [\"user_id\", \"book_id\"], \"left\")\n",
        "\n",
        "user_actual_ratings.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX4EuzPvwhzk"
      },
      "source": [
        "This code performs the training and evaluation of a Decision Tree model for book prediction, and generates recommendations for a specific user based on the trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdcq9hbdvy6D"
      },
      "source": [
        "Spark Session Creation: The code starts by creating a Spark session using SparkSession.builder. This session will be used for interacting with Spark.\n",
        "\n",
        "Feature Vector Assembly: The code defines a VectorAssembler that combines the \"book_id\" and \"user_id\" columns into a single feature vector column called \"features\".\n",
        "\n",
        "Data Transformation: The code applies the VectorAssembler to the dataset using assembler.transform, creating a new DataFrame dataAssembled with the additional \"features\" column.\n",
        "\n",
        "Data Splitting: The code splits the data into training and test sets using randomSplit, with 80% of the data for training (trainingData) and 20% for testing (testData).\n",
        "\n",
        "Model Training: The code trains a Decision Tree model using DecisionTreeClassifier. It sets the \"rating\" column as the label column and the \"features\" column as the features column. The model is trained on the trainingData DataFrame.\n",
        "\n",
        "Model Evaluation: The code makes predictions on the test data using the trained Decision Tree model (dtModel.transform). It then evaluates the model's performance using the RMSE metric (RegressionEvaluator). The calculated RMSE value is printed using print.\n",
        "\n",
        "Single User Recommendation: The code specifies a single user (user_id) for which recommendations need to be generated. It filters the data DataFrame to retrieve the data for the specified user (user_data).\n",
        "\n",
        "User Data Transformation: The code applies the VectorAssembler to the user data using assembler.transform, creating a new DataFrame user_data_assembled with the additional \"features\" column.\n",
        "\n",
        "User Prediction: The code makes predictions for the user data using the trained Decision Tree model (dtModel.transform). The predictions are stored in the user_predictions DataFrame.\n",
        "\n",
        "User Recommendation Display: The code selects the \"user_id\", \"book_id\", and \"prediction\" columns from the user_predictions DataFrame. It then joins this information with the original user data (user_data) to include the actual ratings in the user_actual_ratings DataFrame. Finally, it displays the recommendations for the user using user_actual_ratings.show()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7YR8zRguXxO",
        "outputId": "8b8eb267-6fa6-4e7e-e028-b067cebe3467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest RMSE: 1.09\n",
            "+-------+-------+------+----------+\n",
            "|user_id|book_id|rating|prediction|\n",
            "+-------+-------+------+----------+\n",
            "|   5461|      1|     3|       4.0|\n",
            "|   5461|      2|     4|       4.0|\n",
            "|   5461|      3|     2|       4.0|\n",
            "|   5461|      5|     5|       4.0|\n",
            "|   5461|      7|     5|       4.0|\n",
            "|   5461|      8|     4|       4.0|\n",
            "|   5461|      9|     4|       4.0|\n",
            "|   5461|     10|     3|       4.0|\n",
            "|   5461|     11|     4|       4.0|\n",
            "|   5461|     14|     4|       4.0|\n",
            "|   5461|     15|     5|       4.0|\n",
            "|   5461|     16|     4|       4.0|\n",
            "|   5461|     19|     5|       4.0|\n",
            "|   5461|     22|     4|       4.0|\n",
            "|   5461|     28|     5|       4.0|\n",
            "|   5461|     31|     4|       4.0|\n",
            "|   5461|     32|     5|       4.0|\n",
            "|   5461|     33|     4|       4.0|\n",
            "|   5461|     35|     4|       4.0|\n",
            "|   5461|     37|     4|       4.0|\n",
            "+-------+-------+------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"BookPrediction\").getOrCreate()\n",
        "\n",
        "# Create the feature vector assembler\n",
        "featureCols = [\"book_id\", \"user_id\"]\n",
        "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
        "\n",
        "# Transform the data using the feature vector assembler\n",
        "dataAssembled = assembler.transform(data)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "(trainingData, testData) = dataAssembled.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Train a Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"rating\", featuresCol=\"features\")\n",
        "rfModel = rf.fit(trainingData)\n",
        "\n",
        "# Make predictions on the test data using the Random Forest model\n",
        "rfPredictions = rfModel.transform(testData)\n",
        "\n",
        "# Evaluate the model using RegressionEvaluator\n",
        "evaluator = RegressionEvaluator(labelCol=\"rating\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rfRMSE = evaluator.evaluate(rfPredictions)\n",
        "print(\"Random Forest RMSE: {:.2f}\".format(rfRMSE))\n",
        "\n",
        "# Single user recommendation example\n",
        "user_id = 5461\n",
        "\n",
        "# Filter the data for the given user\n",
        "user_data = data.filter(data.user_id == user_id)\n",
        "\n",
        "# Assemble the features for the user data\n",
        "user_data_assembled = assembler.transform(user_data)\n",
        "\n",
        "# Make predictions for the user using the Random Forest model\n",
        "user_predictions = rfModel.transform(user_data_assembled)\n",
        "\n",
        "# Show the recommendations for the user\n",
        "user_recommendations = user_predictions.select(\"user_id\", \"book_id\", \"prediction\")\n",
        "user_actual_ratings = user_data.join(user_recommendations, [\"user_id\", \"book_id\"], \"left\")\n",
        "\n",
        "user_actual_ratings.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kalz33eAwjKq"
      },
      "source": [
        "This code performs the training and evaluation of a Random Forest model for book prediction, and generates recommendations for a specific user based on the trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFG3iTkbwYDp"
      },
      "source": [
        "Spark Session Creation: The code starts by creating a Spark session using SparkSession.builder. This session will be used for interacting with Spark.\n",
        "\n",
        "Feature Vector Assembly: The code defines a VectorAssembler that combines the \"book_id\" and \"user_id\" columns into a single feature vector column called \"features\".\n",
        "\n",
        "Data Transformation: The code applies the VectorAssembler to the dataset using assembler.transform, creating a new DataFrame dataAssembled with the additional \"features\" column.\n",
        "\n",
        "Data Splitting: The code splits the data into training and test sets using randomSplit, with 80% of the data for training (trainingData) and 20% for testing (testData).\n",
        "\n",
        "Model Training: The code trains a Random Forest model using RandomForestClassifier. It sets the \"rating\" column as the label column and the \"features\" column as the features column. The model is trained on the trainingData DataFrame.\n",
        "\n",
        "Model Evaluation: The code makes predictions on the test data using the trained Random Forest model (rfModel.transform). It then evaluates the model's performance using the RMSE metric (RegressionEvaluator). The calculated RMSE value is printed using print.\n",
        "\n",
        "Single User Recommendation: The code specifies a single user (user_id) for which recommendations need to be generated. It filters the data DataFrame to retrieve the data for the specified user (user_data).\n",
        "\n",
        "User Data Transformation: The code applies the VectorAssembler to the user data using assembler.transform, creating a new DataFrame user_data_assembled with the additional \"features\" column.\n",
        "\n",
        "User Prediction: The code makes predictions for the user data using the trained Random Forest model (rfModel.transform). The predictions are stored in the user_predictions DataFrame.\n",
        "\n",
        "User Recommendation Display: The code selects the \"user_id\", \"book_id\", and \"prediction\" columns from the user_predictions DataFrame. It then joins this information with the original user data (user_data) to include the actual ratings in the user_actual_ratings DataFrame. Finally, it displays the recommendations for the user using user_actual_ratings.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXECUTION TIME FOR COLLABORATIVE FILTERING IN PYSPARK"
      ],
      "metadata": {
        "id": "aQtWAw1-qFlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"BookPrediction\").getOrCreate()\n",
        "\n",
        "# Dividing the data using random split into train_data and test_data in 80% and 20% respectively\n",
        "start_time = time.time()\n",
        "train_data, test_data = data.randomSplit([0.8, 0.2])\n",
        "end_time = time.time()\n",
        "data_split_execution_time = end_time - start_time\n",
        "\n",
        "# Build the recommendation model using ALS on the training data\n",
        "als = ALS(maxIter=5,\n",
        "          regParam=0.01,\n",
        "          userCol=\"user_id\",\n",
        "          itemCol=\"book_id\",\n",
        "          ratingCol=\"rating\")\n",
        "\n",
        "# Fitting the model on the train_data\n",
        "start_time = time.time()\n",
        "model = als.fit(train_data)\n",
        "end_time = time.time()\n",
        "model_training_execution_time = end_time - start_time\n",
        "\n",
        "# Evaluate the model by computing the RMSE on the test data\n",
        "start_time = time.time()\n",
        "predictions = model.transform(test_data)\n",
        "end_time = time.time()\n",
        "prediction_execution_time = end_time - start_time\n",
        "\n",
        "# Printing and calculating RMSE\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
        "start_time = time.time()\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "end_time = time.time()\n",
        "rmse_calculation_execution_time = end_time - start_time\n",
        "\n",
        "# Filtering user with user id \"5461\" with book id on which it has given the reviews\n",
        "user1 = test_data.filter(test_data['user_id'] == 5461).select(['book_id', 'user_id'])\n",
        "\n",
        "# Training and evaluating for user1 with our model trained with the help of training data\n",
        "start_time = time.time()\n",
        "recommendations = model.transform(user1)\n",
        "end_time = time.time()\n",
        "user_recommendation_execution_time = end_time - start_time\n",
        "\n",
        "# Printing the execution times\n",
        "print(\"Data Split Execution Time: {:.2f} seconds\".format(data_split_execution_time))\n",
        "print(\"Model Training Execution Time: {:.2f} seconds\".format(model_training_execution_time))\n",
        "print(\"Prediction Execution Time: {:.2f} seconds\".format(prediction_execution_time))\n",
        "print(\"RMSE Calculation Execution Time: {:.2f} seconds\".format(rmse_calculation_execution_time))\n",
        "print(\"User Recommendation Execution Time: {:.2f} seconds\".format(user_recommendation_execution_time))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zic7jCuUY9E3",
        "outputId": "c4f71edd-284f-4fa1-95d9-89cfc3414657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Split Execution Time: 0.03 seconds\n",
            "Model Training Execution Time: 14.17 seconds\n",
            "Prediction Execution Time: 0.14 seconds\n",
            "RMSE Calculation Execution Time: 4.25 seconds\n",
            "User Recommendation Execution Time: 0.10 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXECUTION TIME FOR DECISION TREE IN PYSPARK\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4947loEwUQyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"BookPrediction\").getOrCreate()\n",
        "\n",
        "# Create the feature vector assembler\n",
        "featureCols = [\"book_id\", \"user_id\"]\n",
        "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
        "\n",
        "# Transform the data using the feature vector assembler\n",
        "dataAssembled = assembler.transform(data)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "(trainingData, testData) = dataAssembled.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Measure the execution time for training the Decision Tree model\n",
        "start_time = time.time()\n",
        "dt = DecisionTreeClassifier(labelCol=\"rating\", featuresCol=\"features\")\n",
        "dtModel = dt.fit(trainingData)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(\"Training Execution Time: {:.2f} seconds\".format(execution_time))\n",
        "\n",
        "# Measure the execution time for making predictions on the test data\n",
        "start_time = time.time()\n",
        "dtPredictions = dtModel.transform(testData)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(\"Prediction Execution Time: {:.2f} seconds\".format(execution_time))\n",
        "\n",
        "# Measure the execution time for the single user recommendation example\n",
        "start_time = time.time()\n",
        "user_id = 5461\n",
        "user_data = data.filter(data.user_id == user_id)\n",
        "user_data_assembled = assembler.transform(user_data)\n",
        "user_predictions = dtModel.transform(user_data_assembled)\n",
        "user_recommendations = user_predictions.select(\"user_id\", \"book_id\", \"prediction\")\n",
        "user_actual_ratings = user_data.join(user_recommendations, [\"user_id\", \"book_id\"], \"left\")\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(\"Single User Recommendation Execution Time: {:.2f} seconds\".format(execution_time))\n"
      ],
      "metadata": {
        "id": "_WxY_NKyWK3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f299d363-d9aa-4887-d7b0-bcccebc85042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Execution Time: 15.81 seconds\n",
            "Prediction Execution Time: 0.75 seconds\n",
            "Single User Recommendation Execution Time: 0.31 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXECUTION TIME FOR RANDOM FOREST IN PYSPARK"
      ],
      "metadata": {
        "id": "rmvQ8T6_Un_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"BookPrediction\").getOrCreate()\n",
        "\n",
        "# Create the feature vector assembler\n",
        "featureCols = [\"book_id\", \"user_id\"]\n",
        "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
        "\n",
        "# Transform the data using the feature vector assembler\n",
        "dataAssembled = assembler.transform(data)  # Replace 'data' with your DataFrame variable\n",
        "\n",
        "# Split the data into training and test sets\n",
        "(trainingData, testData) = dataAssembled.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Measure the execution time for training the Random Forest model\n",
        "start_time = time.time()\n",
        "rf = RandomForestClassifier(labelCol=\"rating\", featuresCol=\"features\")\n",
        "rfModel = rf.fit(trainingData)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(\"Training Execution Time: {:.2f} seconds\".format(execution_time))\n",
        "\n",
        "# Measure the execution time for making predictions on the test data\n",
        "start_time = time.time()\n",
        "rfPredictions = rfModel.transform(testData)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(\"Prediction Execution Time: {:.2f} seconds\".format(execution_time))\n",
        "\n",
        "# Measure the execution time for the single user recommendation example\n",
        "start_time = time.time()\n",
        "user_id = 5461\n",
        "user_data = data.filter(data.user_id == user_id)  # Replace 'data' with your DataFrame variable\n",
        "user_data_assembled = assembler.transform(user_data)\n",
        "user_predictions = rfModel.transform(user_data_assembled)\n",
        "user_recommendations = user_predictions.select(\"user_id\", \"book_id\", \"prediction\")\n",
        "user_actual_ratings = user_data.join(user_recommendations, [\"user_id\", \"book_id\"], \"left\")\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(\"Single User Recommendation Execution Time: {:.2f} seconds\".format(execution_time))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZcR3q3HUnlV",
        "outputId": "e150813f-55ae-4288-8d79-d3649a7da74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Execution Time: 27.09 seconds\n",
            "Prediction Execution Time: 0.24 seconds\n",
            "Single User Recommendation Execution Time: 0.36 seconds\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}